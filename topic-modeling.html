---
layout: default
permalink: topic-modeling
---
<head>
  <style>
    h4 {text-align: center;}
    img.center {
    display: block;
    margin: 0 auto;
    }
  </style>
</head>

<body>
  <h4>
    <a href="https://pnlpuos.github.io/overview">Overview</a><a>&emsp;</a>
    <a href="https://pnlpuos.github.io/data-preparation">Data Preparation</a><a>&emsp;</a>
    <a href="https://pnlpuos.github.io/topic-modeling">Topic Modeling</a><a>&emsp;</a>
    <a href="https://pnlpuos.github.io/sentiment-analysis">Sentiment Analysis</a><a>&emsp;</a>
    <a href="https://pnlpuos.github.io/outcome">Outcome and Reflection</a><a>&emsp;</a>
  </h4>
</body>

<h1>Topic Modeling</h1>
<p>
  <h3>Approach</h3>
  As mentioned in the <a href="https://pnlpuos.github.io/overview">Overview</a>
  section of this report, our topic analysis pipeline employs a document
  clustering approach which dynamically-identifies an optimal clustering of
  vectorized document representations, before passing clustered documents onto
  later tasks such as <a href="https://pnlpuos.github.io/sentiment-analysis">Sentiment
  Analysis</a>. However, it must be admitted that this approach is not the
  most-common method for achieving the task of topic modeling, and we below
  provide an overview of the task of topic modeling to contextualize the
  subsequent deep-dive into our methodology.
</p>
<p>
  Topic modeling is the natural language processing (NLP) task of identifying
  representative groups of topics in a text dataset. In other words, when
  provided a set of documents, an effective topic modeling will demonstrate
  'what the documents are about.' Given that we implement a topic modeling
  pipeline for reporting on open employee survey comments, reasonable topics
  identified by a successful modeling may include topics such as <i>salary</i>,
  <i>teamwork</i>, etc.
</p>
<p>
  There exist two main approaches to the topic modeling task: statistical
  topic model approaches and document clustering approaches. We briefly
  describe the statistical approach before investigating our methods.
</p>

<p>[SUBIR]</p>

<p>
  We employ a document clustering approach given our academic interest in
  exploring the applications of pretrained word embeddings coupled with
  traditional cluster analyses. Furthermore, we ignore supervised approaches
  such as document classification because these are unsuitable for our task
  given the requirement that the pipeline be able to dynamically-identify
  new topics potentially unrepresented in a training set.
</p>
<p>
  The document clustering approach requires that we first transform the
  unstructured comment data into numerical features before
  performing a cluster analysis on the feature-engineered documents. After
  feature-engineering we obtain document vectors for which we perform a
  cluster analysis optimized via hyperparameter grid search, before
  generating meaningful topic labels useful for report generation. The below
  image summarizes the topic modeling pipeline as implemented in our
  codebase.
</p>
<p>
  <img src="https://raw.githubusercontent.com/PNLPUOS/PNLPUOS.github.io/topic_modeling/images/pipeline-topics.png" alt="...">
</p>
<p>
  <h3>Document Vectorization</h3>
  While it is a trivial task to obtain word vectors from pretrained neural word embedding models
  such as word2vec or FastText, it is challenging to devise a methodology for converting these
  word-level vectors into a normalized document representation which still encodes the
  semantic information of the document's concatenated tokens.
</p>
<p>
  Many solutions to the problem of document vectorization leverage the
  demonstrated <i>compositionality</i> of obtained word embeddings, i.e. the behavior that
  larger blocks of information such as phrases can be represented by the vector manipulation of
  embedding constituents. [1] Due to this property of trained word embeddings, one can negotiate
  vector operations such as addition to superficially 'combine' the semantic information
  encoded in individual tokens into a document representation.
</p>
<p>
  A na√Øve solution for obtaining a document vector is the simple average of
  embedding vectors constituting a document. As an initial approach, we employed this method
  for obtaining document vectors, but the resulting clusterings did not yield optimal results.
  The figure below demonstrates a sample clustering on FastText vectors averaged to obtain
  document vectors. Clearly, the clusters are not clearly separable.
</p>
<p>
  <img src="https://raw.githubusercontent.com/PNLPUOS/PNLPUOS.github.io/topic_modeling/images/graph-badclusters.png" alt="..." width="500", class="center">
</p>
<p>
  It has been shown that weighting embeddings via smoothed term frequency can
  improve performance on text similarity tasks without the need for additional training data. [3]
  Because of these findings, we opted to implement additional functionality for computing the
  smooth-inverse-frequency (SIF) weighted average of word embeddings as a document representation.
  The SIF weighting equation for a single word is demonstrated below:
</p>
<p>
  <img src="https://raw.githubusercontent.com/PNLPUOS/PNLPUOS.github.io/topic_modeling/images/eq-sif.png" alt="..." width="200" class="center">
</p>
<p>
  where <i>a</i> denotes an arbitrary normalization constant of approximately 1e-5, and <i>p(w)</i>
  denotes the word's frequency. Nevertheless, preliminary evaluations showed that this baseline,
  although effective for text similarity, did not generalize well to the creation of document
  vectors which performed effectively for document clustering.
</p>
<p>
  Inspired by reseerch into effective document representations for twitter sentiment
  analysis, a domain which is somewhat comparable to ours given similarities between twitter
  texts and our short, focused comments, we opted to integrate into the SIF weighted average the
  Term-Frequency-Inverse-Document-Frequency (TFIDF) measure as a replacement for the value <i>p(w)</i>,
  as was found to be itself effective as a weighting schema for document vectors [4, 5].
  With this modification, the weighting equation for single words in our
  averaged document vectors becomes <br>
<p>
  <img src="https://raw.githubusercontent.com/PNLPUOS/PNLPUOS.github.io/topic_modeling/images/eq-siftfidf.png" alt="..." width="200" class="center">
</p>
<p>
  where <i>w</i> for a term <i>i</i> in document <i>j</i> denotes<br>
</p>
<p>
  <img src="https://www.link-assistant.com/images/news/tf-idf-tool-for-seo/screen-03.png" alt="..." class="center" width="200">
</p>
<p>
  The resulting weighting scheme produces satisfactory clusterings when qualitatively
  compared with other approaches. Seen below, a sample clustering with increased
  separation between identified clusters suggests that the TFIDF weighting scheme captures
  the most-relevant keywords of each document and effectively weights these terms in order
  to generate more meaningfully-distinct document vector representations.
</p>
<p>
  <img src="https://raw.githubusercontent.com/PNLPUOS/PNLPUOS.github.io/topic_modeling/images/graph-goodclusters.png" alt="..." width="500" class="center">
</p>
<p>
  <h3>Dimensionality Reduction</h3>
  ...
<p>
  ...
</p>
</p>

<p>
  <h3>Clustering</h3>
	<h4>Introduction<h4>
		<p>
			Clustering is a method where similar or similar properties of data points are group together
			Given a set of dataset points, we can just use a clustering algorithm to search for pattern in the
			given scenario, finding similarities within the data and group them together.
			<br>
			<br>
			Clustering Algorithm can be divided into Four types :
			<ul>
			  <li>Flat and Centroid based</li>
			  <li>Flat and Density based</li>
			  <li>Hierarchical and Centroid based</li>
			  <li>Hierarchical and Density based</li>
			</ul>
			<br>
			<img src="https://raw.githubusercontent.com/PNLPUOS/PNLPUOS.github.io/topic_modeling/images/clustering_types.png" alt="...">
			<br>

			The above picture shows overall idea of clustering types. Flat and Centroid based alogorithm such as <a href="https://en.wikipedia.org/wiki/K-means_clustering">K-means clustering</a> has prior assumptions on the shape of the clusters. It works very well on defined shapes of clusters such as circular, rectangular, etc also works best when we have small amount, low dimensional data. But when we have higher dimensional data and also no prior knowledge of shapes of cluster, then k-means performs very poorly, and in such case we prefer to use Flat and Density based such as <a href="https://scikit-learn.org/stable/modules/generated/sklearn.cluster.DBSCAN.html">DBSCAN clustering</a> which is short form of Density Based Spatial Clustering. DBSCAN works very best on for clusters of different shapes, variable densities, variable sizes and datasets have noises in data. It's very difficult to judge the number of clusters in a dataset, what can be the best parameter settings for number of clusters? Hierarchical based algorithms solve this problem, Hierarchical and Centroid based algorithm such as <a href="https://jbhender.github.io/Stats506/F18/GP/Group10.html">Agglomertive Hierarchical clustering</a> which splits all the data points and make a dendogram and then clusters are fromed from the branches. Since we are dealing with very high dimensional data with lots of noise, we chose to use <a href = https://hdbscan.readthedocs.io/en/latest/how_hdbscan_works.html> HDBSCAN </a> which is mixture of DBSCAN and Hierarchical based algorithm.
		</p>
	<h4>HDBSCAN</h4>
		<p>
		HDBSCAN works on DBSCAN, DBSCAN has two parameters that are : epsilon and min-points. epsilon is radius of a circle, which enclose data points, and if data points inside 'eps'(epsilon) radius are above K(min-points) then the circle considered as Dense Circle. In HDBSCAN, the parameter min-points are passed and fixed for DBSCAN, and different epsilon radius circle are drawn, and then HDBSCAN algorithm looks for the smaller eps value which accomodate min-points. And that eps value considered as dense. The circles get merged using Probability density function and using distance metrics <a href = "https://hdbscan.readthedocs.io/en/latest/how_hdbscan_works.html#transform-the-space"> Mutual Rechability Distance </a> components get connected forming <a href = "https://hdbscan.readthedocs.io/en/latest/how_hdbscan_works.html#build-the-minimum-spanning-tree">minimum spanning tree</a>. Next step in HDBSCAN is to convert MST into the hierarchy of connected components, using sorting of edges of MST which is also converted in Dendogram. HDBSCAN takes a parameter minimum cluster size, to eradicate clusters that has lower data points than minimum cluster size. Afterwards, HDBSCAN takes another parameter which acts as threshold which cut out the condensed dendogram, the maximum points in the hierarchy considered as clusters. For more info see here <a href = "https://hdbscan.readthedocs.io/en/latest/how_hdbscan_works.html#extract-the-clusters">Cluster Extraction </a>


		</p>


  ...
</p>
</p>

<p>
  <h3>Topic Labeling</h3>
  ...
<p>
  ...
</p>
</p>

<p>
  <h3>Grid Search</h3>
    <h4>The algorithm</h4>
      <body>
        In general a grid searach is nothing else then successively trying out
        different function input parameter configurations then measure the
        performance for each configuration with a predefined score and
        maximizing this score to get the best possible setting for the undelying
        problem. The name grid search comes from the visualization in which each
        of the configurations can be seen as a node of a grid in a high
        dimensional vector space (e.g. a cube in a 3-dimensional parameter
        space).
      </body>

      <h4>Our approach</h4>
      <body>
        The general definition of grid search is scanning the data to configure the
        optimal parameters for a model. In our topic model approach this means
        something slightly different namely finding the best parameters for the
        different components occuring in the topic modeling pipeline
        (e.g. normalization, dimensionality reduction, clustering).<br><br>
        To implement a smooth and dynamic grid search process we had to adjust
        our undelying pipeline architecture in a very standardized but also
        flexible way. This means we gave ever component function the fixed
        structure which requires the data, the name of the used algorithm and the
        parameter configuration. With this structure it is very easy to switch
        drop out specific components or for our case it was very easy to call the
        functions in sequence for the grid search.<br>
        For the actual process we implemented a <b> pipeline </b> variable (Image 1)
        which is basicly a python dictionary which contains all information about the
        grid search components. So it is built up with all different functions
        and a bunch of input parameters which then make the grid: <br><br>
        <img src="https://raw.githubusercontent.com/PNLPUOS/PNLPUOS.github.io/topic_modeling/images/pipeline.png" alt="...">
        <br>
        Image 1: Add normalization, dimensionality reduction and clustering
        algorithm to the pipeline.
        <br><br>
        The acutal grid search was implemented in a very dynamic way fitting the
        pipeline structure. Therefore, it is very easy to a pipeline component
        by adding it to the pipeline variable with the required information.
        This process is possible due to providing the python function for example
        for the clustering algorithm as a kind of object variable so it can just be
        iterated through these successive components. Surely, they have to be
        designed in a way that previous functions output suits the input of the
        upcoming one.<br>
        The score we tried to optimize by our grid search is the so called
        <b>silhouette score</b> which tries to measure how close the points of
        one cluster are compared to the distance between the different clusters.
        By optimizing this score we found the best parameter configuration which
        we could set as default for out topic modeling pipeline. Additionally,
        only for a cross check we also took the number of outliars into account.
      </body>

      <h4>Logging</h4>
        <body>
          Running a grid search without logging the information you get during
          the process is abolutely pointless. Therefore, we paired up each
          configuration with the associated scores and saved them. To make these
          information accessable for everyone and guarantee saving at least a
          partition of the outcome even the grid search fails at any point we
          decided to sent the found pairs to our Mongo data base. With the help
          of the pyhton library <b>pymongo</b> it was extremely easy to set up
          a connection to the data base, creating a python dictionary with all
          important information and then store it in grid search collection.
          The library automatically converts the python dictionary to a MongoDB
          object. Additionally, we implement some sort of wrapper for a smoother
          handling of the information in the data base. This wrapper allowed us
          to write, delete and search for data base objects so that we can
          extract the information about the best configurations again.
        </body>
<p>
  ...
</p>
</p>

<style>
table, th, td {
  border: 1px solid black;
  border-collapse: collapse;
}
</style>
<p>
	<h3>Analysis of the Grid Search Logs</h3>
	<h4>Our Approach</h4>
	In addition to figuring out the best configurations from the grid search descriptively we analyzed the grid search logs by means of a multiple regression analysis in order to assess how much the different parameters  contribute to the silhouette score. The six predictors entered in the regression were parameters of the grid search defining the functioning of the UMAP dimensionality reduction algorithm and the hdbscan clustering algorithm. For both algorithms their metric (canberra/cosine/euclidean and canberra/euclidean, respectively) was entered as a predictor. Additionally, for UMAP the number of neighbours and the spread were taken into account and for hdbscan the minimum cluster size as well as the minimum number of samples. All predictors are treated as categorical data as only a few distinct values of the normally continuous parameters could be taken into account during the grid search because of the limited computational power available. <br>
As the grid search was run twice, once for the pipeline based on Fasttext embeddings and the other time based on BERT embeddings, two analyses were conducted and are presented separately. Our goal with both analyses was to find a straightforward solution employing only predictors with a high contribution to the explained variance while nevertheless explaining as much variance as possible. The dependent variable is the silhouette score, the outliers were not included as a dependent variable. However, in order to reduce noise in the data, we excluded results with more than 10% outliers from the analyses. Additionally, results presenting more than 375 clusters were also excluded, as such a high number of clusters seemed unreasonable given our input data.
</p>
<p>
        <h4>Results with Fasttext embeddings</h4>
        We identified a model employing the ordinary least squares (OLS) technique with four predictors and no interactions, all entered as fixed effects, that accounts for 67.24% of the variance in our data (<I>F</I>(5, 624) = 259.9, <I>p</I> < 0.001, R&sup2 = 0.6755, Adjusted R&sup2 = 0.6729). The results suggest that a combination of the cosine metric for UMAP, a higher number of neighbors for UMAP and a higher cluster size as well as a smaller number of samples for the hdbscan clustering algorithm are advisable in order to get a high silhouette score (<I>Table 1</I>).
	<br><br>
	<I>Table 1: Results of the multiple regression analysis.<br>
	Significance codes: '***' 0.001 &nbsp;&nbsp;&nbsp;'**' 0.01 &nbsp;&nbsp;&nbsp;'*' 0.05 <br>
	R&sup2 = 0.6755, Adjusted R&sup2 = 0.6729 <br>
	AIC = -2532.875 </I><br>
        <table>
  		<tr>
    			<th> </th>
    			<th>Estimate</th>
    			<th>Std.Error</th>
			<th>t-value</th>
  		</tr>
  		<tr>
   			<td>Intercept</td>
    			<td>-0.19768</td>
    			<td>0.00463</td>
			<td>-42.731 &nbsp&nbsp***</td>
  		</tr>
  		<tr>
    			<td>UMAP: metric cosine</td>
    			<td>0.08805</td>
    			<td>0.00314</td>
			<td>28.080 &nbsp&nbsp***</td>
  		</tr>
  		<tr>
    			<td>UMAP: metric euclidean</td>
    			<td>0.02838</td>
    			<td>0.00314</td>
			<td>9.050 &nbsp&nbsp***</td>
  		</tr>
  		<tr>
    			<td>UMAP: number of neighbors</td>
    			<td>0.00105</td>
    			<td> 0.00010 </td>
			<td>10.267 &nbsp&nbsp***</td>
  		</tr>
  		<tr>
    			<td>hdbscan: min cluster size</td>
    			<td>0.00068</td>
    			<td>0.00004</td>
			<td>17.929 &nbsp&nbsp***</td>
  		</tr>
  		<tr>
    			<td>hdbscan: min number of samples</td>
    			<td>-0.00102</td>
    			<td>0.00033</td>
			<td>-3.085 &nbsp&nbsp**</td>
  		</tr>
	</table>
</p>
<p>
	<h4>Results with BERT embeddings</h4>
	For the grid search logs that were generated using the pipeline with BERT embeddings the results look different. As the precondition of homoscedasticity was not satisfied, we employed a regression with the generalised least squares technique (GLS). The best model here comprises three predictors and one interaction, all entered as fixes effects. The implied advice here is to use the euclidean metric for both algorithms and additionally have a higher minimum cluster size for the hdbscan algorithm (<I>Table 2</I>).
	<br><br>
	<I>Table 2: Results of the multiple regression analysis.<br>
	Significance codes: '***' 0.001 &nbsp;&nbsp;&nbsp;'**' 0.01 &nbsp;&nbsp;&nbsp;'*' 0.05 <br>
	AIC = -1186.359 BIC = -1147.218 logLik = 601.1795 </I><br>
        <table>
  		<tr>
    			<th> </th>
    			<th>Value</th>
    			<th>Std.Error</th>
			<th>t-value</th>
  		</tr>
  		<tr>
   			<td>Intercept</td>
    			<td>-0.10262</td>
    			<td>0.01211</td>
			<td>-8.474 &nbsp&nbsp***</td>
  		</tr>
  		<tr>
    			<td>UMAP: metric cosine</td>
    			<td>0.00123</td>
    			<td>0.01415</td>
			<td>0.087 &nbsp&nbsp n.s.</td>
  		</tr>
  		<tr>
    			<td>UMAP: metric euclidean</td>
    			<td>0.04785</td>
    			<td>0.014146</td>
			<td>3.383 &nbsp&nbsp***</td>
  		</tr>
  		<tr>
    			<td>hdbscan: metric euclidean</td>
    			<td>0.05686</td>
    			<td>0.01412</td>
			<td>4.026 &nbsp&nbsp***</td>
  		</tr>
  		<tr>
    			<td>hdbscan: min cluster size</td>
    			<td>0.00163</td>
    			<td>0.00012</td>
			<td>14.117 &nbsp&nbsp***</td>
  		</tr>
  		<tr>
    			<td>Interaction: UMAP metric cosine x hdbscan metric euclidean</td>
    			<td>-0.02125</td>
    			<td>0.01999</td>
			<td>-1.063 &nbsp&nbsp n.s.</td>
  		</tr>
  		<tr>
    			<td>Interaction: UMAP metric euclidean x hdbscan metric euclidean</td>
    			<td>0.16369</td>
    			<td>0.01997</td>
			<td>8.195 &nbsp&nbsp***</td>
  		</tr>
	</table>
</p>
<p>
	<h4>Conclusion</h4>
	Both regressions imply simple advice on how to optimize the configurations in the pipeline, nevertheless there are limitations to these results. Most importantly, as from a theoretical point of view only a certain range of the number of clusters was to be expected, the results of a grid search where the possible number of clusters would be limited to the sensible range could add more insights. Furthermore, the regression models do not fit the data perfectly, which can partly be explained by the noise in the original data.
<p>
  ...
</p>
</p>

<p>
  <h3>References</h3>
  <a>&emsp;</a>[1] Mikolov, Tomas & Corrado, G.s & Chen, Kai & Dean, Jeffrey. (2013). Efficient Estimation of Word Representations in Vector Space. 1-12.<br>
  <a>&emsp;</a>[2] P. Bojanowski, E. Grave, A. Joulin, T. Mikolov. (2016) Enriching Word Vectors with Subword Information.<br>
  <a>&emsp;</a>[3] Arora, S., Liang, Y., & Ma, T. (2016). A simple but tough-to-beat baseline for sentence embeddings. Paper presented at 5th International Conference on Learning Representations. <br>
  <a>&emsp;</a>[4] Edilson Anselmo Correa Junior, Vanessa Marinho, and Leandro Santos. (2017). NILC-USP at SemEval-2017 Task 4: A Multi-view Ensemble for Twitter Sentiment Analysis. In Proceedings of the 11th International Workshop on Semantic Evaluation. Vancouver, Canada, SemEval ‚Äô17, pages 610‚Äì614.<br>
  <a>&emsp;</a>[5] Zhao, Jiang & Lan, Man & Tian, Jun. (2015). ECNU: Using Traditional Similarity Measurements and Word Embedding for Semantic Textual Similarity Estimation.<br>
</p>
